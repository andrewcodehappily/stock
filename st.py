# Copyright 2026 andrewcodehappily
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import streamlit as st
import datetime as dt
import time
import requests
import pandas as pd
import numpy as np
import yfinance as yf
import xml.etree.ElementTree as ET
from dateutil import parser
from dateutil.relativedelta import relativedelta
from io import StringIO
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
import os
import re # ç”¨ä¾†åšæ­£è¦è¡¨é”å¼æª¢æŸ¥ï¼Œé˜²æ³¨å…¥

# ==========================================
# 0. å…¨åŸŸè¨­å®š & Utils
# ==========================================
st.set_page_config(page_title="ğŸ‡¹ğŸ‡¼ å°è‚¡å…¨æ–¹ä½å„€è¡¨æ¿", layout="wide", page_icon="ğŸ“ˆ")

# ğŸ’€ è‡´å‘½æ¼æ´ä¸€ä¿®å¾©ï¼šå¯†ç¢¼ä¸å†è£¸å¥”
# å˜—è©¦å¾ st.secrets ç²å–å¯†ç¢¼ï¼Œå¦‚æœæ²’æœ‰è¨­å®š (æœ¬æ©Ÿé–‹ç™¼æ™‚)ï¼Œç‚ºäº†ä¸è®“ä½ è·‘ä¸å‹•ï¼Œ
# æˆ‘é‚„æ˜¯å¾—ç•™å€‹å¾Œé–€ï¼Œä½†é€™æ¬¡æˆ‘æœƒåŠ ä¸Šå¤§å¤§å¤§å¤§çš„è­¦å‘Šï¼
try:
    VALID_KEYS = st.secrets.get("valid_keys", ["vn781326"])
except FileNotFoundError:
    # æœ¬æ©Ÿæ²’è¨­å®š .streamlit/secrets.toml æ™‚çš„ fallback
    VALID_KEYS = ["vn781326"] 
    # é€™è£¡æˆ‘å€‘å¿ƒçŸ¥è‚šæ˜å°±å¥½ï¼Œæ­£å¼ä¸Šç·šè«‹ä¸€å®šè¦ç”¨ secrets.toml

def get_today_taipei() -> dt.datetime:
    """ç²å–ç•¶å‰å°ç£æ™‚é–“"""
    try:
        return dt.datetime.now(dt.timezone.utc).astimezone(dt.timezone(dt.timedelta(hours=8)))
    except Exception:
        return dt.datetime.now()

def calculate_default_start_date() -> str:
    """è¨ˆç®—ä¸¦è¿”å›ä¸€å¹´å‰çš„æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰"""
    today = get_today_taipei().date()
    one_year_ago = today - relativedelta(years=1)
    return one_year_ago.strftime('%Y-%m-%d')

def add_watermark(fig, text=""):
    """ç‚º Plotly åœ–è¡¨æ·»åŠ æµ®æ°´å°"""
    if not text: return fig
    fig.add_annotation(
        text=text,
        xref="paper", yref="paper",
        x=0.5, y=0.5,
        showarrow=False,
        font=dict(family="Arial, sans-serif", size=60, color="rgba(128,128,128,0.1)"),
        yanchor="middle", xanchor="center",
        opacity=0.2,
    )
    return fig

# ğŸ¤ é‚è¼¯æ¼æ´ä¿®å¾©ï¼šä¸ç•¶é´•é³¥ï¼Œæœ‰éŒ¯è¦å–Šå‡ºä¾†
def log_error(debug_log, message):
    """çµ±ä¸€éŒ¯èª¤ç´€éŒ„"""
    timestamp = get_today_taipei().strftime("%H:%M:%S")
    log_msg = f"[{timestamp}] {message}"
    if debug_log is not None:
        debug_log.append(log_msg)
    # åœ¨çµ‚ç«¯æ©Ÿä¹Ÿå°å‡ºä¾†ï¼Œæ–¹ä¾¿ä½ ç½µæˆ‘
    print(f"ğŸ”¥ Error: {message}")

# ==========================================
# 1. è³‡æ–™ç²å–æ¨¡çµ„ (Data)
# ==========================================

@st.cache_data(ttl=3600)
def get_official_daily(stock_id):
    """å¾è­‰äº¤æ‰€ç²å–ä»Šæ—¥ç›¤å¾Œæ•¸æ“š"""
    try:
        today_str = dt.date.today().strftime("%Y%m%d")
        url = f"https://www.twse.com.tw/rwd/zh/afterTrading/STOCK_DAY?date={today_str}&stockNo={stock_id}"
        r = requests.get(url, timeout=10)
        j = r.json()
        if j.get("stat") != "OK": return pd.DataFrame()
        return pd.DataFrame(data=j["data"], columns=j["fields"])
    except Exception as e:
        print(f"get_official_daily error: {e}")
        return pd.DataFrame()

@st.cache_data(ttl=3600)
def get_official_monthly_valuation(stock_id, months=3):
    """ç²å–æœˆä¼°å€¼æ•¸æ“š (PE, PB, Yield)"""
    twse_df = pd.DataFrame()
    try:
        now = dt.datetime.now()
        date_list = [(now - relativedelta(months=i)).replace(day=1).strftime("%Y%m%d") for i in range(months)]
        date_list.reverse()
        for date in date_list:
            url = f"https://www.twse.com.tw/rwd/zh/afterTrading/BWIBBU?date={date}&stockNo={stock_id}"
            try:
                r = requests.get(url, timeout=5)
                j = r.json()
                if j.get("stat") == "OK" and j.get("data"):
                    df = pd.DataFrame(data=j["data"], columns=j["fields"])
                    if not df.empty:
                         df = df.iloc[[0]]
                         twse_df = pd.concat([twse_df, df], ignore_index=True)
            except: pass
        if not twse_df.empty:
            return twse_df.sort_values(by=twse_df.columns[0], ascending=False).drop_duplicates()
    except: pass
    
    try:
        tpex_url = f"https://www.tpex.org.tw/web/stock/aftertrading/peratio_dq/peratio_dqa_result.jsp?stkno={stock_id}&l=zh-tw"
        r_tpex = requests.get(tpex_url, timeout=10)
        j_tpex = r_tpex.json()
        if j_tpex.get("iTotalRecords", 0) > 0 and j_tpex.get("aaData"):
            data = j_tpex["aaData"]
            columns = ["è³‡æ–™æ—¥æœŸ", "è­‰åˆ¸ä»£è™Ÿ", "è­‰åˆ¸åç¨±", "æœ¬ç›Šæ¯”", "æ®–åˆ©ç‡(%)", "è‚¡åƒ¹æ·¨å€¼æ¯”", "è²¡å ±å¹´/å­£"]
            df_tpex = pd.DataFrame(data, columns=columns)
            def convert_roc(val):
                p = val.split('/')
                return f"{int(p[0])+1911}/{p[1]}/{p[2]}" if len(p)==3 else val
            df_tpex['è³‡æ–™æ—¥æœŸ'] = df_tpex['è³‡æ–™æ—¥æœŸ'].apply(convert_roc)
            return df_tpex
    except: pass
    return pd.DataFrame()

@st.cache_data(ttl=3600)
def get_market_package_data(stock_id, period_str="1y", custom_start_date=None):
    """
    ç²å–å¸‚å ´æ•¸æ“šåŒ… (åŒ…å«æ­·å²è‚¡åƒ¹ã€å…¬å¸è³‡è¨Šã€è²¡å ±)
    """
    # ğŸ’£ è‡´å‘½æ¼æ´äºŒä¿®å¾©ï¼šè¼¸å…¥æ¡†é˜²å‘† & é˜²æ³¨å…¥
    # ç¢ºä¿ stock_id åªæœ‰æ•¸å­—ï¼Œä¸”é•·åº¦åˆç†
    if not stock_id.isdigit() or len(stock_id) > 6:
        return pd.DataFrame(), {}, pd.DataFrame(), pd.DataFrame(), ["éŒ¯èª¤ï¼šè‚¡ç¥¨ä»£è™Ÿæ ¼å¼ä¸æ­£ç¢º (Input Validation Failed)"]

    stock_id = str(stock_id).strip()
    MAX_RETRIES = 3
    start_date_naive = None
    debug_log = []
    
    if custom_start_date:
        try: start_date_naive = pd.to_datetime(custom_start_date)
        except: pass
    yf_period = '2y'

    for attempt in range(MAX_RETRIES):
        hist = pd.DataFrame()
        yf_ticker = None
        info = {}
        bs = pd.DataFrame()
        is_ = pd.DataFrame()
        source = ""
        try:
            debug_log.append(f"--- å˜—è©¦ç²å–æ•¸æ“š (ç¬¬ {attempt + 1} æ¬¡) ---")
            
            # 1. å˜—è©¦ yfinance (ä¸Šå¸‚)
            ticker = f"{stock_id}.TW"
            s = yf.Ticker(ticker)
            hist = s.history(period=yf_period, auto_adjust=False).reset_index()
            
            # 2. å˜—è©¦ yfinance (ä¸Šæ«ƒ)
            if hist.empty:
                ticker = f"{stock_id}.TWO"
                s = yf.Ticker(ticker)
                hist = s.history(period=yf_period, auto_adjust=False).reset_index()
                source = "yfinance (.TWO)"
            else: 
                source = "yfinance (.TW)"
            
            if not hist.empty: yf_ticker = s
            
            # 3. å˜—è©¦ TPEx API
            if hist.empty and source == "yfinance (.TWO)":
                url = f"https://www.tpex.org.tw/web/stock/aftertrading/daily_trading_info/st43_result.php?l=zh-tw&d=&stkno={stock_id}"
                r = requests.get(url, timeout=10)
                j = r.json()
                source = "TPEx API"
                if j.get("aaData"):
                    df = pd.DataFrame(j["aaData"], columns=["æ—¥æœŸ", "æˆäº¤è‚¡æ•¸", "æˆäº¤é‡‘é¡", "é–‹ç›¤åƒ¹", "æœ€é«˜åƒ¹", "æœ€ä½åƒ¹", "æ”¶ç›¤åƒ¹", "æ¼²è·Œ", "ç­†æ•¸"])
                    def convert_roc(val):
                         p = val.split('/')
                         return f"{int(p[0])+1911}-{p[1]}-{p[2]}" if len(p)==3 else None
                    df["æ—¥æœŸ"] = df["æ—¥æœŸ"].apply(convert_roc)
                    df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors="coerce")
                    for c in ["é–‹ç›¤åƒ¹", "æœ€é«˜åƒ¹", "æœ€ä½åƒ¹", "æ”¶ç›¤åƒ¹", "æˆäº¤è‚¡æ•¸"]:
                        df[c] = pd.to_numeric(df[c].astype(str).str.replace(",", ""), errors="coerce")
                    df.rename(columns={"æ—¥æœŸ": "Date", "é–‹ç›¤åƒ¹": "Open", "æœ€é«˜åƒ¹": "High", "æœ€ä½åƒ¹": "Low", "æ”¶ç›¤åƒ¹": "Close", "æˆäº¤è‚¡æ•¸": "Volume"}, inplace=True)
                    df["Adj_Close"] = df["Close"]
                    hist = df[["Date", "Open", "High", "Low", "Close", "Adj_Close", "Volume"]]
            
            # 4. çµ‚æ¥µå‚™ç”¨æ–¹æ¡ˆï¼šTWSE STOCK_DAY (åŠ å¼·ç‰ˆ)
            if hist.empty and len(stock_id) == 4 and stock_id.isdigit():
                debug_log.append("è³‡è¨Š: å•Ÿå‹• TWSE å‚™ç”¨æ•‘æ´æ¨¡å¼ (STOCK_DAY)")
                now = dt.date.today()
                start_back = now - relativedelta(years=2)
                cur = now
                backup_hist = pd.DataFrame()
                
                while cur >= start_back:
                    d_str = cur.strftime("%Y%m01")
                    url = f"https://www.twse.com.tw/rwd/zh/afterTrading/STOCK_DAY?date={d_str}&stockNo={stock_id}&response=json"
                    try:
                        r = requests.get(url, timeout=5)
                        j = r.json()
                        if j.get('stat') == 'OK' and j.get('data'):
                            fields = j.get('fields', [])
                            df_m = pd.DataFrame(j['data'], columns=fields)
                            
                            col_map = {}
                            for col in fields:
                                col_lower = col.lower()
                                if "æ—¥æœŸ" in col or "date" in col_lower: col_map[col] = "Date"
                                elif "æˆäº¤è‚¡æ•¸" in col or "volume" in col_lower or "shares" in col_lower: col_map[col] = "Volume"
                                elif "é–‹ç›¤" in col or "open" in col_lower: col_map[col] = "Open"
                                elif "æœ€é«˜" in col or "high" in col_lower: col_map[col] = "High"
                                elif "æœ€ä½" in col or "low" in col_lower: col_map[col] = "Low"
                                elif "æ”¶ç›¤" in col or "close" in col_lower: col_map[col] = "Close"
                            
                            df_m.rename(columns=col_map, inplace=True)
                            df_m['Date'] = df_m['Date'].str.replace('/', '-').apply(
                                lambda x: str(int(x.split('-')[0]) + 1911) + '-' + x.split('-')[1] + '-' + x.split('-')[2]
                            )
                            
                            for c in ['Volume', 'Open', 'High', 'Low', 'Close']:
                                if c in df_m.columns: 
                                    df_m[c] = pd.to_numeric(df_m[c].astype(str).str.replace(',', '').str.replace('X', '').str.strip(), errors='coerce')
                            
                            df_m["Adj_Close"] = df_m["Close"]
                            needed_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Adj_Close', 'Volume']
                            available_cols = [c for c in needed_cols if c in df_m.columns]
                            if available_cols:
                                backup_hist = pd.concat([backup_hist, df_m[available_cols]], ignore_index=True)
                        cur = cur.replace(day=1) - relativedelta(days=1)
                        time.sleep(0.5) 
                    except Exception as e: 
                        log_error(debug_log, f"TWSE å‚™ä»½æŠ“å–éŒ¯èª¤: {e}")
                        break
                
                if not backup_hist.empty:
                    backup_hist['Date'] = pd.to_datetime(backup_hist['Date'])
                    backup_hist.drop_duplicates(subset=['Date'], inplace=True)
                    hist = backup_hist.sort_values('Date', ascending=True).reset_index(drop=True)
                    source = "TWSE Backup"

            if yf_ticker:
                try:
                    info = yf_ticker.info
                    bs = yf_ticker.balance_sheet
                    is_ = yf_ticker.financials
                    if not bs.empty: bs.columns = bs.columns.astype(str)
                    if not is_.empty: is_.columns = is_.columns.astype(str)
                except: pass

            if not hist.empty:
                if "Adj Close" in hist.columns: hist.rename(columns={"Adj Close": "Adj_Close"}, inplace=True)
                elif "Adj_Close" not in hist.columns: hist["Adj_Close"] = hist["Close"]
                
                if not pd.api.types.is_datetime64_any_dtype(hist['Date']):
                    hist['Date'] = pd.to_datetime(hist['Date'], utc=True).dt.tz_localize(None)
                elif hist['Date'].dt.tz is not None:
                     hist['Date'] = hist['Date'].dt.tz_localize(None)
                
                hist = hist[hist['Date'] > pd.to_datetime('2000-01-01')]
                hist.dropna(subset=["Date", "Close"], inplace=True)
                
                # --- é˜²å‘†æª¢æŸ¥ï¼šé¿å…ã€Œçˆ¬æ¨“æ¢¯ã€æ•¸æ“š ---
                if len(hist) > 10:
                    prices = hist['Close'].values
                    is_straight_line = np.all(np.diff(prices) == 1) or np.all(np.diff(prices) == -1)
                    if is_straight_line or prices[-1] < 0.1:
                        log_error(debug_log, "âš ï¸ è­¦å‘Šï¼šåµæ¸¬åˆ°ç•°å¸¸è‚¡åƒ¹æ•¸æ“š (å¯èƒ½æ˜¯ç´¢å¼•éŒ¯èª¤)ï¼Œå·²æ¨æ£„ã€‚")
                        hist = pd.DataFrame() 
                    else:
                        if start_date_naive:
                            hist_filtered = hist[hist['Date'] >= start_date_naive].sort_values(by='Date', ascending=True).reset_index(drop=True)
                            if not hist_filtered.empty: hist = hist_filtered
                        
                        for c in ["Open", "High", "Low", "Close", "Adj_Close"]:
                            if c in hist.columns: hist[c] = hist[c].round(2)
                        if "Volume" in hist.columns: hist["Volume"] = hist["Volume"].fillna(0).astype(int)
                        
                        hist["Date"] = hist["Date"].dt.strftime("%Y-%m-%d")
                        return hist, info, bs, is_, debug_log
            
            time.sleep(1)
                
        except Exception as e:
            log_error(debug_log, f"å˜—è©¦å¤±æ•—: {str(e)}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(1)
                continue
                
    return pd.DataFrame(), {}, pd.DataFrame(), pd.DataFrame(), debug_log

@st.cache_data(ttl=3600)
def get_stock_news(stock_id, stock_name=""):
    """ä½¿ç”¨ Google News RSS"""
    try:
        query = f"{stock_id} {stock_name}"
        rss_url = f"https://news.google.com/rss/search?q={query}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(rss_url, headers=headers, timeout=10)
        root = ET.fromstring(response.content)
        news_data = []
        for item in root.findall('./channel/item')[:15]:
            title = item.find('title').text if item.find('title') is not None else 'N/A'
            link = item.find('link').text if item.find('link') is not None else 'N/A'
            pub_date = item.find('pubDate').text if item.find('pubDate') is not None else 'N/A'
            source = item.find('source').text if item.find('source') is not None else 'N/A'
            try:
                dt_obj = parser.parse(pub_date)
                pub_date_str = dt_obj.strftime('%Y-%m-%d %H:%M:%S')
            except: pub_date_str = pub_date
            news_data.append({"ç™¼å¸ƒæ™‚é–“": pub_date_str, "æ¨™é¡Œ": title, "ä¾†æº": source, "é€£çµ": link})
        if news_data: return pd.DataFrame(news_data)
        return pd.DataFrame()
    except Exception as e: return pd.DataFrame()

@st.cache_data(ttl=3600)
def get_institutions_net_change(market_type: str) -> pd.DataFrame:
    """ç²å–ä¸‰å¤§æ³•äººè²·è³£è¶…"""
    try:
        now = dt.datetime.now(dt.timezone.utc).astimezone(dt.timezone(dt.timedelta(hours=8)))
        for i in range(5):
            target_date = now - dt.timedelta(days=i)
            today_str = target_date.strftime("%Y%m%d")
            if market_type == 'ä¸Šå¸‚':
                url = f"https://www.twse.com.tw/rwd/zh/fund/T86?date={today_str}&selectType=ALLBUT0999&response=json"
                r = requests.get(url, timeout=10)
                j = r.json()
                if j.get('stat') == 'OK' and j.get('data'):
                    data = []
                    for row in j['data']:
                        try: data.append({'ä»£è™Ÿ': row[0], 'å¤–è³‡_Net': float(row[4].replace(',', '')), 'æŠ•ä¿¡_Net': float(row[7].replace(',', '')), 'è‡ªç‡Ÿå•†_Net': float(row[10].replace(',', ''))})
                        except: continue
                    return pd.DataFrame(data)
            elif market_type == 'ä¸Šæ«ƒ':
                tpex_date_str = f"{target_date.year-1911}/{target_date.month:02d}/{target_date.day:02d}"
                url = f"https://www.tpex.org.tw/web/stock/3insti/daily_trade/3itrade_hedge_result.php?l=zh-tw&o=json&se=EW&t=D&d={tpex_date_str}"
                r = requests.get(url, timeout=10)
                j = r.json()
                if j.get('aaData'):
                    data = []
                    for row in j['aaData']:
                        try: data.append({'ä»£è™Ÿ': row[0], 'å¤–è³‡_Net': float(row[7].replace(',', '')), 'æŠ•ä¿¡_Net': float(row[10].replace(',', '')), 'è‡ªç‡Ÿå•†_Net': float(row[13].replace(',', ''))})
                        except: continue 
                    return pd.DataFrame(data)
            time.sleep(0.5)
    except: pass
    return pd.DataFrame()

@st.cache_data(ttl=3600)
def get_margin_trading_data(market_type: str) -> pd.DataFrame:
    """ç²å–èè³‡èåˆ¸æ•¸æ“š"""
    try:
        now = dt.datetime.now(dt.timezone.utc).astimezone(dt.timezone(dt.timedelta(hours=8)))
        for i in range(5):
            target_date = now - dt.timedelta(days=i)
            today_str = target_date.strftime("%Y%m%d")
            if market_type == 'ä¸Šå¸‚':
                url = f"https://www.twse.com.tw/rwd/zh/marginTrading/MI_MARGN?date={today_str}&selectType=ALL&response=json"
                r = requests.get(url, timeout=10)
                j = r.json()
                if j.get('stat') == 'OK' and j.get('data'):
                    data = []
                    for row in j['data']:
                        try:
                            curr = float(row[6].replace(',', '')); prev = float(row[5].replace(',', ''))
                            data.append({'ä»£è™Ÿ': row[0], 'èè³‡_Net': curr - prev})
                        except: continue
                    return pd.DataFrame(data)
            elif market_type == 'ä¸Šæ«ƒ':
                tpex_date_str = f"{target_date.year-1911}/{target_date.month:02d}/{target_date.day:02d}"
                url = f"https://www.tpex.org.tw/web/stock/margin_trading/margin_balance/margin_bal_result.php?l=zh-tw&o=json&d={tpex_date_str}"
                r = requests.get(url, timeout=10)
                j = r.json()
                if j.get('aaData'):
                    data = []
                    for row in j['aaData']:
                        try:
                            curr = float(row[6].replace(',', '')); prev = float(row[2].replace(',', ''))
                            data.append({'ä»£è™Ÿ': row[0], 'èè³‡_Net': curr - prev})
                        except: continue
                    return pd.DataFrame(data)
            time.sleep(0.5)
    except: pass
    return pd.DataFrame()

@st.cache_data(ttl=3600)
def get_market_screener_data_base(market_type: str) -> pd.DataFrame:
    """ç²å–é¸è‚¡ç”¨çš„åŸºæœ¬é¢æ•¸æ“š (PE, Yield, PB)"""
    try:
        now = dt.datetime.now(dt.timezone.utc).astimezone(dt.timezone(dt.timedelta(hours=8)))
        for i in range(5):
            target_date = now - dt.timedelta(days=i)
            d_str = target_date.strftime("%Y%m%d")
            if market_type == 'ä¸Šå¸‚':
                url = f"https://www.twse.com.tw/rwd/zh/afterTrading/BWIBBU?date={d_str}&selectType=ALL&response=json"
                r = requests.get(url, timeout=10)
                j = r.json()
                if j.get('stat') == 'OK':
                    df = pd.DataFrame(j['data'], columns=j['fields'])
                    df.columns = ['ä»£è™Ÿ', 'åç¨±', 'PE', 'Yield', 'PB', 'Report_Q']
                    for col in ['PE', 'Yield', 'PB']:
                        df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', '').str.replace('-', 'nan'), errors='coerce')
                    return df[['ä»£è™Ÿ', 'åç¨±', 'PE', 'Yield', 'PB']]
            elif market_type == 'ä¸Šæ«ƒ':
                 tpex_date_str = f"{int(d_str[:4])-1911}/{d_str[4:6]}/{d_str[6:]}"
                 url = f"https://www.tpex.org.tw/web/stock/aftertrading/peratio_dq/peratio_dqa_result.jsp?l=zh-tw&o=json&d={tpex_date_str}&c=ALL"
                 r = requests.get(url, timeout=10)
                 j = r.json()
                 if j.get('aaData'):
                     data = []
                     for row in j['aaData']:
                         try: data.append({'ä»£è™Ÿ': row[0], 'åç¨±': row[1], 'PE': float(row[2].replace(',', '')), 'Yield': float(row[3].replace(',', '')), 'PB': float(row[4].replace(',', ''))})
                         except: continue
                     return pd.DataFrame(data)
            time.sleep(1)
    except: pass
    return pd.DataFrame()

def check_technicals(stock_id, ma_cond, rsi_cond):
    """æŠ€è¡“é¢ç¯©é¸æª¢æŸ¥"""
    if ma_cond == 'ä¸é™' and rsi_cond == 'ä¸é™': return True
    df, _, _, _, _ = get_market_package_data(stock_id, period_str="3mo", custom_start_date=None)
    if df.empty: return False
    
    # ğŸ’£ è‡´å‘½æ¼æ´äºŒä¿®å¾©ï¼šMA åƒæ•¸é˜²å‘†
    # åš´æ ¼æª¢æŸ¥ MA åƒæ•¸ï¼Œé˜²æ­¢äº‚è¼¸å…¥å°è‡´ crash
    df = get_technical_indicators(df, "5,20,60", "12,26,9", 14, price_col='Close')
    if len(df) < 5: return False
    last_row = df.iloc[-1]
    price = last_row['Close']
    if ma_cond != 'ä¸é™':
        if ma_cond == 'Price > MA5' and price <= last_row.get('SMA_5', 0): return False
        if ma_cond == 'Price > MA20' and price <= last_row.get('SMA_20', 0): return False
        if ma_cond == 'Price > MA60' and price <= last_row.get('SMA_60', 0): return False
        if ma_cond == 'Price < MA5' and price >= last_row.get('SMA_5', 99999): return False
        if ma_cond == 'Price < MA20' and price >= last_row.get('SMA_20', 99999): return False
    if rsi_cond != 'ä¸é™':
        rsi = last_row.get('RSI', 50)
        if rsi_cond == 'RSI > 50' and rsi <= 50: return False
        if rsi_cond == 'RSI < 50' and rsi >= 50: return False
        if rsi_cond == 'RSI > 70 (è¶…è²·)' and rsi <= 70: return False
        if rsi_cond == 'RSI < 30 (è¶…è³£)' and rsi >= 30: return False
    return True

def get_market_screener_data(market_type, pe_min, pe_max, yield_min, f_trend, t_trend, d_trend, m_trend, ta_ma, ta_rsi, progress=None):
    """é¸è‚¡ä¸»é‚è¼¯"""
    df_val = get_market_screener_data_base(market_type)
    if df_val.empty: return pd.DataFrame({"è³‡è¨Š": ["ç„¡æ³•ç²å–ä¼°å€¼æ•¸æ“šæˆ–ä»Šæ—¥éäº¤æ˜“æ—¥"]})
    df_inst = get_institutions_net_change(market_type)
    df_marg = get_margin_trading_data(market_type)
    if not df_inst.empty: df = pd.merge(df_val, df_inst, on='ä»£è™Ÿ', how='left')
    else: df = df_val; df['å¤–è³‡_Net'] = 0; df['æŠ•ä¿¡_Net'] = 0; df['è‡ªç‡Ÿå•†_Net'] = 0
    if not df_marg.empty: df = pd.merge(df, df_marg, on='ä»£è™Ÿ', how='left')
    else: df['èè³‡_Net'] = 0
    df.fillna(0, inplace=True)
    mask = (df['PE'] >= pe_min) & (df['PE'] <= pe_max) & (df['Yield'] >= yield_min)
    df_filtered = df[mask].copy()
    if f_trend == 'å¢åŠ ': df_filtered = df_filtered[df_filtered['å¤–è³‡_Net'] > 0]
    elif f_trend == 'æ¸›å°‘': df_filtered = df_filtered[df_filtered['å¤–è³‡_Net'] < 0]
    if t_trend == 'å¢åŠ ': df_filtered = df_filtered[df_filtered['æŠ•ä¿¡_Net'] > 0]
    elif t_trend == 'æ¸›å°‘': df_filtered = df_filtered[df_filtered['æŠ•ä¿¡_Net'] < 0]
    if d_trend == 'å¢åŠ ': df_filtered = df_filtered[df_filtered['è‡ªç‡Ÿå•†_Net'] > 0]
    elif d_trend == 'æ¸›å°‘': df_filtered = df_filtered[df_filtered['è‡ªç‡Ÿå•†_Net'] < 0]
    if m_trend == 'å¢åŠ ': df_filtered = df_filtered[df_filtered['èè³‡_Net'] > 0]
    elif m_trend == 'æ¸›å°‘': df_filtered = df_filtered[df_filtered['èè³‡_Net'] < 0]
    if df_filtered.empty: return pd.DataFrame({"è³‡è¨Š": ["åŸºæœ¬é¢/ç±Œç¢¼é¢ç„¡ç¬¦åˆæ¢ä»¶è‚¡ç¥¨"]})
    
    if ta_ma != 'ä¸é™' or ta_rsi != 'ä¸é™':
        candidates = df_filtered.sort_values(by='Yield', ascending=False)
        # ğŸ¢ æ•ˆèƒ½æ¼æ´ä¿®å¾©ï¼šè¿´åœˆåœ°ç„
        # é™åˆ¶ç¯©é¸æ•¸é‡ä¸Šé™ï¼Œé¿å… DoS æ”»æ“Š
        LIMIT_CANDIDATES = 10 
        candidates = candidates.head(LIMIT_CANDIDATES) 
        
        final_list = []
        my_bar = st.progress(0, text="æ­£åœ¨é€²è¡ŒæŠ€è¡“é¢ç¯©é¸...")
        total_c = len(candidates)
        for i, (idx, row) in enumerate(candidates.iterrows()):
            my_bar.progress((i + 1) / total_c, text=f"æ­£åœ¨åˆ†æ: {row['ä»£è™Ÿ']}...")
            
            # ğŸ¢ æ•ˆèƒ½æ¼æ´ä¿®å¾©ï¼šåŠ å…¥å»¶é²ï¼Œå°è­‰äº¤æ‰€æº«æŸ”ä¸€é»
            time.sleep(1.5) 
            
            if check_technicals(row['ä»£è™Ÿ'], ta_ma, ta_rsi): final_list.append(row)
        my_bar.empty()
        if not final_list: return pd.DataFrame({"è³‡è¨Š": ["æŠ€è¡“é¢ç¯©é¸å¾Œç„¡ç¬¦åˆæ¢ä»¶è‚¡ç¥¨"]})
        df_filtered = pd.DataFrame(final_list)
        
    for c in ['å¤–è³‡_Net', 'æŠ•ä¿¡_Net', 'è‡ªç‡Ÿå•†_Net', 'èè³‡_Net']:
        if c in df_filtered.columns: df_filtered[c] = df_filtered[c].apply(lambda x: f"{x:,.0f}")
    cols = ['ä»£è™Ÿ', 'åç¨±', 'PE', 'Yield', 'PB', 'å¤–è³‡_Net', 'æŠ•ä¿¡_Net', 'è‡ªç‡Ÿå•†_Net', 'èè³‡_Net']
    return df_filtered[cols].sort_values(by='Yield', ascending=False)

@st.cache_data(ttl=3600)
def get_tdcc_opendata(stock_id):
    """å¾é›†ä¿ Open Data ç²å–è³‡æ–™"""
    url = "https://smart.tdcc.com.tw/opendata/getOD.ashx?id=1-5"
    try:
        s = requests.Session()
        s.headers.update({'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'})
        r = s.get(url, timeout=30)
        if r.status_code == 200:
             try: text = r.content.decode('utf-8')
             except: text = r.content.decode('big5', errors='ignore')
             df = pd.read_csv(StringIO(text))
             col_id = next((c for c in df.columns if "ä»£è™Ÿ" in c), None)
             col_level = next((c for c in df.columns if "åˆ†ç´š" in c), None)
             col_people = next((c for c in df.columns if "äººæ•¸" in c), None)
             col_shares = next((c for c in df.columns if "è‚¡æ•¸" in c), None)
             col_date = next((c for c in df.columns if "æ—¥æœŸ" in c), None)
             if col_id and col_level and col_shares:
                 target = df[df[col_id].astype(str).str.strip() == str(stock_id)]
                 if not target.empty:
                     if col_date:
                         d_str = str(target.iloc[0][col_date])
                         if len(d_str) == 8: date_val = f"{d_str[:4]}-{d_str[4:6]}-{d_str[6:]}"
                         else: date_val = str(dt.date.today())
                     else: date_val = str(dt.date.today())
                     week_data = {"Date": date_val}
                     for _, row in target.iterrows():
                         try:
                             lvl = int(row[col_level])
                             shares = int(row[col_shares])
                             if col_people: people = int(row[col_people])
                             else: people = 0
                             if 1 <= lvl <= 15:
                                 label = TDCC_BRACKETS[lvl-1]
                                 week_data[label] = shares
                                 week_data[f"{label}_People"] = people
                         except: continue
                     return pd.DataFrame([week_data])
    except Exception as e: print(f"Open Data Error: {e}")
    return pd.DataFrame()

TDCC_BRACKETS = ["1-999è‚¡", "1-5å¼µ", "5-10å¼µ", "10-15å¼µ", "15-20å¼µ", "20-30å¼µ", "30-40å¼µ", "40-50å¼µ", "50-100å¼µ", "100-200å¼µ", "200-400å¼µ", "400-600å¼µ", "600-800å¼µ", "800-1000å¼µ", "1000å¼µä»¥ä¸Š"]

def calculate_tdcc_holding_value(tdcc_df, price_df, selected_brackets, threshold_amount=None):
    if tdcc_df.empty or price_df.empty: return pd.DataFrame({"è³‡è¨Š": ["ç„¡æ•¸æ“šã€‚"]})
    tdcc_df['Date'] = pd.to_datetime(tdcc_df['Date'])
    price_df['Date'] = pd.to_datetime(price_df['Date'])
    bracket_min_shares = {"1-999è‚¡": 0, "1-5å¼µ": 1000, "5-10å¼µ": 5000, "10-15å¼µ": 10000, "15-20å¼µ": 15000, "20-30å¼µ": 20000, "30-40å¼µ": 30000, "40-50å¼µ": 40000, "50-100å¼µ": 50000, "100-200å¼µ": 100000, "200-400å¼µ": 200000, "400-600å¼µ": 400000, "600-800å¼µ": 600000, "800-1000å¼µ": 800000, "1000å¼µä»¥ä¸Š": 1000000}
    brackets_list = list(bracket_min_shares.keys())
    price_subset = price_df[['Date', 'Close']].sort_values('Date')
    tdcc_df = tdcc_df.sort_values('Date')
    merged = pd.merge_asof(tdcc_df, price_subset, on='Date', direction='backward')
    result_data = []
    for idx, row in merged.iterrows():
        price = row.get('Close', np.nan)
        if pd.isna(price): continue
        row_dict = {"æ—¥æœŸ": row['Date'].strftime('%Y-%m-%d'), "æ”¶ç›¤åƒ¹": price}
        for col in selected_brackets:
            if col in brackets_list and col in row and pd.notna(row[col]):
                val = row[col] * price / 10000 
                row_dict[col] = f"{int(val):,}"
        if threshold_amount is not None and threshold_amount > 0:
            target_shares = (threshold_amount * 10000) / price
            start_idx = 0; found = False
            for i, b_name in enumerate(brackets_list):
                 if bracket_min_shares[b_name] > target_shares: start_idx = max(0, i - 1); found = True; break
            if not found: start_idx = len(brackets_list) - 1
            above_keys = brackets_list[start_idx:]
            valid_above = [k for k in above_keys if k in row]
            val_above = sum([row[k] * price / 10000 for k in valid_above if pd.notna(row[k])])
            people_above = 0
            for k in valid_above:
                 p_key = f"{k}_People"
                 if p_key in row and pd.notna(row[p_key]): people_above += row[p_key]
            row_dict[f"å¤§æˆ¶(>={int(threshold_amount)}è¬)äººæ•¸"] = f"{int(people_above):,}"
            row_dict[f"å¤§æˆ¶(>={int(threshold_amount)}è¬)é‡‘é¡(è¬)"] = f"{int(val_above):,}"
        result_data.append(row_dict)
    return pd.DataFrame(result_data)

# ==========================================
# 2. åˆ†ææ¨¡çµ„ (Analysis)
# ==========================================

def get_technical_indicators(df, ma_lengths_str, macd_params_str, rsi_length, price_col='Close'):
    if df.empty or "Close" not in df.columns: return pd.DataFrame()
    df = df.copy()
    if 'Date' in df.columns: df['Date'] = pd.to_datetime(df['Date'])
    if price_col not in df.columns: price_col = 'Close'
    try: ma_lengths = [int(x.strip()) for x in ma_lengths_str.split(",")]
    except: ma_lengths = [5, 10, 20, 60, 100, 200]
    for length in ma_lengths: df[f"SMA_{length}"] = df[price_col].rolling(window=length).mean()
    try: fast, slow, signal = [int(x.strip()) for x in macd_params_str.split(",")]
    except: fast, slow, signal = 12, 26, 9
    ema_fast = df[price_col].ewm(span=fast, adjust=False).mean()
    ema_slow = df[price_col].ewm(span=slow, adjust=False).mean()
    df["MACD"] = ema_fast - ema_slow
    df["MACD_Signal"] = df["MACD"].ewm(span=signal, adjust=False).mean()
    df["SMA_20_BB"] = df[price_col].rolling(window=20).mean()
    df["StdDev"] = df[price_col].rolling(window=20).std()
    df["Bollinger_Upper"] = df["SMA_20_BB"] + 2 * df["StdDev"]
    df["Bollinger_Lower"] = df["SMA_20_BB"] - 2 * df["StdDev"]
    low_14 = df["Low"].rolling(window=14).min(); high_14 = df["High"].rolling(window=14).max()
    df["KD_K"] = 100 * ((df[price_col] - low_14) / (high_14 - low_14))
    df["KD_D"] = df["KD_K"].rolling(window=3).mean()
    try: rlen = int(rsi_length)
    except: rlen = 14
    delta = df[price_col].diff()
    gain = (delta.where(delta > 0, 0)).ewm(alpha=1 / rlen, adjust=False).mean()
    loss = (-delta.where(delta < 0, 0)).ewm(alpha=1 / rlen, adjust=False).mean()
    rs = gain / loss
    df["RSI"] = 100 - (100 / (1 + rs))
    if not pd.api.types.is_string_dtype(df['Date']): df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')
    return df

def calculate_fundamental_ratios(info, bs, income):
    try:
        ratios = {
            "å¸‚å€¼": f"{info.get('marketCap', 'N/A'):,.0f}" if isinstance(info.get('marketCap'), (int, float)) else "N/A",
            "æœ¬ç›Šæ¯”": f"{info.get('trailingPE', 'N/A'):.2f}" if isinstance(info.get('trailingPE'), (int, float)) else "N/A",
            "EPS": f"{info.get('trailingEps', 'N/A'):.2f}" if isinstance(info.get('trailingEps'), (int, float)) else "N/A",
            "ROE": f"{info.get('returnOnEquity', 'N/A'):.2%}" if isinstance(info.get('returnOnEquity'), float) else "N/A",
        }
        return pd.DataFrame(list(ratios.items()), columns=["æ¯”ç‡", "æ•¸å€¼"])
    except: return pd.DataFrame({"éŒ¯èª¤": ["è¨ˆç®—å¤±æ•—"]})

def predict_stock_price(df_historical, predict_days=5, stock_id="", stock_name="", currency="TWD"):
    """
    è‚¡åƒ¹é æ¸¬æ¨¡å‹ 2.0 (Ridge + Volatility Interval)
    """
    summary = "è³‡æ–™ä¸è¶³ï¼Œç„¡æ³•é æ¸¬ã€‚"
    future_plot = None
    if df_historical.empty: return summary, future_plot
    data = df_historical.dropna().copy()
    if len(data) < 30: return summary, future_plot
    if 'Date' in data.columns: data['Date'] = pd.to_datetime(data['Date'])
    target_col = 'Adj_Close' if 'Adj_Close' in data.columns else 'Close'

    # ç‰¹å¾µå·¥ç¨‹
    data['Days'] = (data['Date'] - data['Date'].min()).dt.days
    data['MA_Short'] = data[target_col].rolling(5).mean()
    data['MA_Long'] = data[target_col].rolling(20).mean()
    data['Momentum'] = data[target_col].diff(5)
    data = data.dropna()
    features = ['Days', 'MA_Short', 'MA_Long', 'Momentum']
    if len(data) < 30: return summary, future_plot

    X = data[features].values
    y = data[target_col].values.reshape(-1, 1)

    scaler_x = StandardScaler()
    scaler_y = StandardScaler()
    X_scaled = scaler_x.fit_transform(X)
    y_scaled = scaler_y.fit_transform(y)
    weights = np.exp(np.linspace(0, 3, len(data))) 
    model = Ridge(alpha=0.1)
    model.fit(X_scaled, y_scaled, sample_weight=weights)
    
    # è¨ˆç®—è¿‘æœŸæ³¢å‹•ç‡ (ATR-like approximation)
    recent_volatility = data[target_col].diff().std() * 1.5 # 1.5å€æ¨™æº–å·®ä½œç‚ºå€é–“
    
    last_row_raw = X[-1].reshape(1, -1).copy()
    last_date = data['Date'].iloc[-1]
    last_days = data['Days'].iloc[-1]
    
    preds = []; preds_upper = []; preds_lower = []
    future_dates = []
    idx_days = 0; idx_ma_short = 1

    cur_date = last_date + pd.Timedelta(days=1)
    # é æ¸¬æœªä¾†
    step_vol = 0
    while len(future_dates) < predict_days:
        if cur_date.weekday() < 5:
            step_vol += 1 # éš¨è‘—æ™‚é–“å¢åŠ ä¸ç¢ºå®šæ€§
            days_diff = (cur_date - last_date).days
            last_row_raw[0][idx_days] = last_days + days_diff
            curr_scaled = scaler_x.transform(last_row_raw)
            pred_val = model.predict(curr_scaled)
            val = pred_val[0][0] if pred_val.ndim > 1 else pred_val[0]
            val_arr = np.array(val).reshape(1, -1)
            pred_raw = round(scaler_y.inverse_transform(val_arr)[0][0], 2)
            
            preds.append(pred_raw)
            # åŠ å…¥æ³¢å‹•å€é–“ (Reference Value UP!)
            uncertainty = recent_volatility * np.sqrt(step_vol)
            preds_upper.append(pred_raw + uncertainty)
            preds_lower.append(pred_raw - uncertainty)
            
            future_dates.append(cur_date.strftime("%Y-%m-%d"))
            new_short = (last_row_raw[0][idx_ma_short] * 4 + pred_raw) / 5
            last_row_raw[0][idx_ma_short] = new_short
        cur_date += pd.Timedelta(days=1)
        
    fig = go.Figure()
    # æ­·å²è‚¡åƒ¹
    fig.add_trace(go.Scatter(x=data['Date'], y=data[target_col], mode="lines", name=f"æ­·å² ({target_col})", line=dict(color='#1f77b4')))
    
    # é æ¸¬å€é–“ (Cloud)
    fig.add_trace(go.Scatter(
        x=future_dates+future_dates[::-1], 
        y=preds_upper+preds_lower[::-1], 
        fill='toself', 
        fillcolor='rgba(255, 0, 0, 0.2)', 
        line=dict(color='rgba(255,255,255,0)'), 
        hoverinfo="skip", 
        showlegend=True, 
        name='é æ¸¬å€é–“ (æ¨‚è§€/æ‚²è§€)'
    ))
    
    # é æ¸¬ä¸­ç·š
    pred_txt = [f"{p:.2f}" for p in preds]
    fig.add_trace(go.Scatter(x=future_dates, y=preds, mode="lines+markers+text", name=f"é æ¸¬ä¸­ä½æ•¸", text=pred_txt, textposition="top center", line=dict(color='red', dash='dot')))
    
    fig.update_layout(title=f"{stock_id} è‚¡åƒ¹é æ¸¬ (å«æ³¢å‹•å€é–“)", xaxis_title="æ—¥æœŸ", yaxis_title="åƒ¹æ ¼")
    fig = add_watermark(fig, stock_id) # åªé¡¯ç¤º Stock ID
    
    last_p = float(y[-1][0])
    first_p = preds[0]
    pct = (first_p - last_p) / last_p * 100
    d = "æ¼²" if pct > 0 else "è·Œ"
    
    summary = f"""
    **åŸºæº–åƒ¹æ ¼:** {last_p:.2f} {currency}
    
    **ğŸ¤– AI é æ¸¬æ‘˜è¦:**
    - **é æ¸¬ä¸­ä½:** {first_p:.2f} ({d} {abs(pct):.2f}%)
    - **æ¨‚è§€æƒ…å¢ƒ:** {preds_upper[0]:.2f}
    - **æ‚²è§€æƒ…å¢ƒ:** {preds_lower[0]:.2f}
    
    *è¨»ï¼šå€é–“åŸºæ–¼è¿‘æœŸæ³¢å‹•ç‡æ¨ç®—ï¼Œåƒ…ä¾›å¨›æ¨‚ï¼ŒæŠ•è³‡è«‹è‡ªè² ç›ˆè™§ã€‚*
    """
    return summary, fig

def plot_technical_analysis(df, indicators, stock_id, ma_str, heights_str):
    if df.empty: return None
    df_display = get_technical_indicators(df, ma_str, "12,26,9", 14, price_col='Close')
    try: mas = [int(x) for x in ma_str.split(",")]
    except: mas = [5, 10, 20]
    subplots = [i for i in indicators if i in ["MACD", "KD", "RSI", "æˆäº¤é‡"]]
    rows = 1 + len(subplots)
    try:
        custom_heights = [float(x.strip()) for x in heights_str.split(",")]
        if len(custom_heights) == rows: row_heights = custom_heights
        else: raise ValueError
    except:
        default = 0.6
        sub = (1.0 - default) / len(subplots) if subplots else 0.4
        row_heights = [default] + [sub] * len(subplots)
    if sum(row_heights) > 0: row_heights = [h/sum(row_heights) for h in row_heights]
    
    fig = make_subplots(rows=rows, cols=1, shared_xaxes=True, row_heights=row_heights, vertical_spacing=0.03)
    fig.add_trace(go.Candlestick(x=df_display['Date'], open=df_display['Open'], high=df_display['High'], low=df_display['Low'], close=df_display['Close'], name="Kç·š"), row=1, col=1)
    for ma in mas:
        if f"SMA_{ma}" in df_display.columns:
            fig.add_trace(go.Scatter(x=df_display['Date'], y=df_display[f"SMA_{ma}"], name=f"MA{ma}", line=dict(width=1)), row=1, col=1)
    if "Bollinger_Upper" in df_display.columns and "å¸ƒæ—é€šé“" in indicators:
        fig.add_trace(go.Scatter(x=df_display['Date'], y=df_display["Bollinger_Upper"], name="BB Up", line=dict(width=1, color='gray')), row=1, col=1)
        fig.add_trace(go.Scatter(x=df_display['Date'], y=df_display["Bollinger_Lower"], name="BB Low", line=dict(width=1, color='gray'), fill='tonexty'), row=1, col=1)
    
    r = 2
    for ind in subplots:
        if ind == "æˆäº¤é‡": fig.add_trace(go.Bar(x=df_display['Date'], y=df_display['Volume'], name="Vol"), row=r, col=1)
        elif ind == "KD" and "KD_K" in df_display.columns:
            fig.add_trace(go.Scatter(x=df_display['Date'], y=df_display['KD_K'], name="K"), row=r, col=1)
            fig.add_trace(go.Scatter(x=df_display['Date'], y=df_display['KD_D'], name="D"), row=r, col=1)
        elif ind == "MACD" and "MACD" in df_display.columns:
             fig.add_trace(go.Bar(x=df_display['Date'], y=df_display["MACD"]-df_display["MACD_Signal"], name="Hist"), row=r, col=1)
             fig.add_trace(go.Scatter(x=df_display['Date'], y=df_display["MACD"], name="MACD"), row=r, col=1)
             fig.add_trace(go.Scatter(x=df_display['Date'], y=df_display["MACD_Signal"], name="Sig"), row=r, col=1)
        elif ind == "RSI" and "RSI" in df_display.columns:
             fig.add_trace(go.Scatter(x=df_display['Date'], y=df_display["RSI"], name="RSI"), row=r, col=1)
        r += 1
    fig.update_layout(title=f"{stock_id} æŠ€è¡“åˆ†æ", height=800, xaxis_rangeslider_visible=False)
    fig = add_watermark(fig, stock_id)
    return fig

# ==========================================
# 3. Streamlit ä¸»ç¨‹å¼
# ==========================================

# --- Sidebar Inputs ---
with st.sidebar:
    st.title("ğŸ”‘ æ§åˆ¶å°")
    api_key = st.text_input("æˆæ¬Šé‡‘é‘° (License Key)", type="password", help="è«‹è¼¸å…¥ç®¡ç†å“¡æä¾›çš„é‡‘é‘°")
    
    st.header("è‚¡ç¥¨è¨­å®š")
    stock_id = st.text_input("è‚¡ç¥¨ä»£è™Ÿ", value="2330", max_chars=10)
    start_date = st.text_input("èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)", value=calculate_default_start_date())
    
    st.markdown("---")
    
    # æ–°å¢å´é‚Šæ¬„è­¦èª
    st.markdown("""
    ### âš ï¸ æŠ•è³‡è­¦èª (Disclaimer)
    - **æœ¬ç³»çµ±åƒ…ä¾› Python ç¨‹å¼é–‹ç™¼èˆ‡å­¸è¡“ç ”ç©¶æ¸¬è©¦ç”¨é€”ã€‚**
    - å…§å«ä¹‹ AI é æ¸¬æ¨¡å‹åŠæ‰€æœ‰ç±Œç¢¼æ•¸æ“šå‡å–è‡ªç¬¬ä¸‰æ–¹å…¬é–‹ä¾†æºï¼Œä¸ä¿è­‰å…¶æ­£ç¢ºæ€§èˆ‡å³æ™‚æ€§ã€‚
    - æ‰€æä¾›ä¹‹è³‡è¨Šä¸æ§‹æˆä»»ä½•æŠ•è³‡å»ºè­°ï¼ŒæŠ•è³‡äººæ‡‰ç¨ç«‹åˆ¤æ–·ä¸¦è‡ªè² æå®³è³ å„Ÿè²¬ä»»ã€‚
    - éå¾€ç¸¾æ•ˆä¸ä»£è¡¨æœªä¾†è¡¨ç¾ï¼Œæ“ä½œå‰è«‹è«®è©¢å°ˆæ¥­è²¡å‹™é¡§å•ã€‚
    """)

# --- Main Content ---
if api_key not in VALID_KEYS:
    st.warning("âš ï¸ æˆæ¬Šå¤±æ•—ï¼è«‹è¼¸å…¥æ­£ç¢ºçš„é‡‘é‘°æ‰èƒ½è§£é–é€™å°è®Šå½¢é‡‘å‰›ã€‚")
    st.stop()

# ==========================================
# ğŸ›‘ å¼·åˆ¶å…è²¬è²æ˜æª¢æŸ¥é» (Checkpoint)
# ==========================================
st.markdown("### ğŸ“œ ä½¿ç”¨å‰è«‹å…ˆç°½ç½²ã€Œå…è²¬è²æ˜ã€")

with st.expander("âš ï¸ é»æ“Šå±•é–‹è©³é–±æ¢æ¬¾ (è«‹å‹™å¿…ä»”ç´°é–±è®€)", expanded=True):
    st.markdown("""
    ### ğŸ“œ æœå‹™æ¢æ¬¾èˆ‡æŠ•è³‡é¢¨éšªå…è²¬è²æ˜ (å®Œæ•´ç‰ˆ)

    **ç¬¬ä¸€æ¢ï¼šéæŠ•è³‡å»ºè­°è²æ˜ (No Investment Advice)**
    æœ¬æ‡‰ç”¨ç¨‹å¼ï¼ˆä»¥ä¸‹ç°¡ç¨±ã€Œæœ¬ç³»çµ±ã€ï¼‰æ‰€æä¾›ä¹‹æ‰€æœ‰è³‡è¨Šï¼ŒåŒ…æ‹¬ä½†ä¸é™æ–¼å³æ™‚è‚¡åƒ¹ã€è²¡å‹™å ±è¡¨ã€æŠ€è¡“æŒ‡æ¨™åˆ†æã€ä¸‰å¤§æ³•äººç±Œç¢¼æ•¸æ“šã€AI é æ¸¬æ¨¡å‹çµæœåŠé¸è‚¡ç¯©é¸çµæœï¼Œ**åƒ…ä¾›å­¸è¡“ç ”ç©¶ã€æ•™è‚²è¨“ç·´åŠç¨‹å¼åŠŸèƒ½é–‹ç™¼æ¸¬è©¦ç”¨é€”**ã€‚
    æœ¬ç³»çµ±**ä¸æ§‹æˆ**ä»»ä½•å½¢å¼çš„æŠ•è³‡å»ºè­°ã€è²¡å‹™è¦åŠƒè«®è©¢æˆ–è²·è³£æ¨è–¦ã€‚
    æœ¬ç³»çµ±é–‹ç™¼è€…ä¸å…·å‚™è­‰åˆ¸æŠ•è³‡é¡§å•è³‡æ ¼ï¼Œäº¦æœªç²å¾—ä¸»ç®¡æ©Ÿé—œè¨±å¯é€²è¡ŒæŠ•é¡§æ¥­å‹™ã€‚
    ä½¿ç”¨è€…æ‡‰çŸ¥æ‚‰æ‰€æœ‰æ•¸æ“šå‡ç‚ºåƒè€ƒæ€§è³ªï¼Œä»»ä½•æŠ•è³‡è¡Œç‚ºå‡æ‡‰å°‹æ±‚åˆæ ¼å°ˆæ¥­ä¹‹ç†è²¡é¡§å•æˆ–è­‰åˆ¸ç¶“ç´€å•†ä¹‹å»ºè­°ã€‚

    **ç¬¬äºŒæ¢ï¼šè³‡è¨Šæº–ç¢ºæ€§èˆ‡ç³»çµ±å»¶é² (Data Accuracy & Latency)**
    æœ¬ç³»çµ±ä¹‹æ•¸æ“šä¾†æºå‡å–è‡ªç¬¬ä¸‰æ–¹å…¬é–‹ API æˆ–å…¬é–‹è³‡è¨Šä¾†æºï¼ˆå¦‚ TWSEã€TPExã€Yahoo Finance ç­‰ï¼‰ã€‚
    æœ¬ç³»çµ±**ä¸ä¿è­‰**è³‡è¨Šä¹‹æº–ç¢ºæ€§ã€å³æ™‚æ€§ã€å®Œæ•´æ€§ã€æ­£ç¢ºæ€§æˆ–æœ‰æ•ˆæ€§ã€‚
    ç”±æ–¼ç¶²è·¯é€£ç·šã€è³‡æ–™æºç•°å‹•ã€API é™åˆ¶æˆ–ç³»çµ±é‹ç®—èª¤å·®ï¼Œæ•¸æ“šå¯èƒ½ç”¢ç”Ÿå»¶é²ã€éºæ¼ã€éŒ¯èª¤æˆ–èˆ‡å¯¦éš›ç›¤å‹¢ä¸ç¬¦ä¹‹æƒ…æ³ã€‚
    æœ¬ç³»çµ±å…§å»ºä¹‹ã€ŒAI é æ¸¬ã€åŠã€Œé¸è‚¡ç¯©é¸ã€ä¿‚åŸºæ–¼æ­·å²æ•¸æ“šä¹‹æ•¸å­¸æ¨¡å‹çµ±è¨ˆçµæœï¼Œæ­·å²ç¸¾æ•ˆ**çµ•ä¸ä¿è­‰**æœªä¾†è¡¨ç¾ï¼Œé æ¸¬å€¼åƒ…ä¾›é‚è¼¯æ¼”ç·´ï¼Œä¸å¾—è¦–ç‚ºç²åˆ©ä¿è­‰ã€‚

    **ç¬¬ä¸‰æ¢ï¼šæŠ•è³‡é¢¨éšªæ­éœ² (Risk Disclosure)**
    è­‰åˆ¸åŠç›¸é—œé‡‘èå•†å“æŠ•è³‡å…·æœ‰æ¥µé«˜é¢¨éšªã€‚
    å¸‚å ´æ³¢å‹•å¯èƒ½å°è‡´æŠ•å…¥è³‡æœ¬çš„éƒ¨åˆ†æˆ–å…¨éƒ¨æå¤±ï¼Œç”šè‡³ç”¢ç”Ÿè¶…éåˆå§‹ä¿è­‰é‡‘ä¹‹æå¤±ã€‚
    ä½¿ç”¨è€…æ‡‰å……åˆ†äº†è§£å¸‚å ´ä¹‹æ³¢å‹•æ€§ï¼Œä¸¦å…·å‚™ç¨ç«‹åˆ¤æ–·ä¹‹èƒ½åŠ›ï¼Œå®Œå…¨ç†è§£ä¸¦ç¨ç«‹æ‰¿æ“”æ‰€æœ‰äº¤æ˜“é¢¨éšªã€‚
    ä½¿ç”¨è€…ä¸æ‡‰å°‡æœ¬ç³»çµ±ä¹‹é æ¸¬æ•¸æ“šè¦–ç‚ºçµ•å°åƒè€ƒæŒ‡æ¨™ï¼Œå¸‚å ´æƒ…ç·’ã€ç¸½é«”ç¶“æ¿Ÿã€åœ°ç·£æ”¿æ²»åŠçªç™¼é‡å¤§è¨Šæ¯ç­‰éé‡åŒ–å› ç´ ï¼Œå‡ä¸åœ¨æœ¬ç³»çµ±è€ƒé‡ç¯„åœå…§ã€‚

    **ç¬¬å››æ¢ï¼šè²¬ä»»é™åˆ¶èˆ‡æå®³è³ å„Ÿ (Limitation of Liability)**
    åœ¨æ³•å¾‹å…è¨±çš„æœ€å¤§ç¯„åœå…§ï¼Œé–‹ç™¼è€…å°æ–¼ä½¿ç”¨è€…å› ä½¿ç”¨æˆ–ç„¡æ³•ä½¿ç”¨æœ¬ç³»çµ±æ‰€å°è‡´ä¹‹ä»»ä½•ç›´æ¥ã€é–“æ¥ã€é™„å¸¶ã€ç‰¹åˆ¥ã€æ‡²ç½°æ€§æˆ–è¡ç”Ÿæ€§æå¤±ï¼ˆåŒ…æ‹¬ä½†ä¸é™æ–¼é‡‘éŒ¢è™§æã€åˆ©æ½¤æå¤±ã€è³‡æ–™éºå¤±ã€å•†è­½å—ææˆ–é›»è…¦ç³»çµ±æå£ï¼‰ï¼Œ**å‡ä¸è² ä»»ä½•æå®³è³ å„Ÿè²¬ä»»**ã€‚
    å³ä¾¿é–‹ç™¼è€…æ›¾è¢«å‘ŠçŸ¥è©²ç­‰æå®³ç™¼ç”Ÿä¹‹å¯èƒ½æ€§ï¼Œæœ¬å…è²¬æ¢æ¬¾ä¾ç„¶æœ‰æ•ˆã€‚
    è‹¥ä½¿ç”¨è€…å› åƒè€ƒæœ¬ç³»çµ±è³‡è¨Šè€Œé€²è¡Œä»»ä½•æ±ºç­–ä¸¦å°è‡´è³‡ç”¢æ¸›æï¼Œé–‹ç™¼è€…ä¸è² ä»»ä½•æ³•å¾‹é€£å¸¶è²¬ä»»ã€‚

    **ç¬¬äº”æ¢ï¼šä½¿ç”¨è€…åŒæ„æ¢æ¬¾ (Acceptance of Terms)**
    ç•¶æ‚¨é–‹å§‹ä½¿ç”¨æœ¬å„€è¡¨æ¿æˆ–é»é¸ã€ŒåŒæ„/Agreeã€æŒ‰éˆ•æ™‚ï¼Œå³è¡¨ç¤ºæ‚¨å·²è©³é–±ã€ç†è§£ä¸¦ç„¡æ¢ä»¶åŒæ„æœ¬å…è²¬è²æ˜ä¹‹æ‰€æœ‰å…§å®¹ï¼š
    1. æ‚¨åŒæ„è‡ªè² æŠ•è³‡ç›ˆè™§ï¼Œæ”¾æ£„å°é–‹ç™¼è€…é€²è¡Œä»»ä½•å½¢å¼çš„æ³•å¾‹è¿½ç©¶ã€‚
    2. æ‚¨åŒæ„æœ¬ç³»çµ±åƒ…ä½œç‚ºæ‚¨å­¸ç¿’æ•¸æ“šåˆ†æå·¥å…·ä¹‹ç”¨ã€‚
    3. æ‚¨ç¢ºèªå·²å…·å‚™å®Œå…¨è¡Œç‚ºèƒ½åŠ›ï¼Œèƒ½å°è‡ªèº«è²¡ç”¢æ±ºç­–è² è²¬ã€‚

    **ç¬¬å…­æ¢ï¼šæº–æ“šæ³•èˆ‡ç®¡è½„æ³•é™¢ (Governing Law & Jurisdiction)**
    æœ¬å…è²¬è²æ˜ä¹‹è§£é‡‹èˆ‡é©ç”¨ï¼Œä»¥åŠèˆ‡æœ¬ç³»çµ±æœ‰é—œä¹‹ä»»ä½•çˆ­è­°ï¼Œå‡æ‡‰ä»¥ä¸­è¯æ°‘åœ‹æ³•å¾‹ç‚ºæº–æ“šæ³•ï¼Œä¸¦ä»¥è‡ºç£è‡ºåŒ—åœ°æ–¹æ³•é™¢ç‚ºç¬¬ä¸€å¯©ç®¡è½„æ³•é™¢ã€‚
    """)

# ğŸ§Ÿâ€â™‚ï¸ æ®­å±æ¼æ´ä¿®å¾©ï¼šç”¨ st.session_state è¨˜ä½åŒæ„ç‹€æ…‹
if "agreed" not in st.session_state:
    st.session_state.agreed = False

def agree_callback():
    st.session_state.agreed = True

agree_disclaimer = st.checkbox(
    "æˆ‘å·²è©³é–±ä¸¦åŒæ„ä¸Šè¿°å…è²¬è²æ˜ï¼Œäº†è§£æœ¬ç³»çµ±åƒ…ä¾›ç ”ç©¶ç”¨é€”ï¼Œä¸¦é¡˜è‡ªè² æ‰€æœ‰æŠ•è³‡é¢¨éšªã€‚",
    value=st.session_state.agreed,
    on_change=agree_callback
)

if not agree_disclaimer:
    st.info("ğŸ‘† è«‹å‹¾é¸ä¸Šæ–¹åŒæ„æ¡†ï¼Œæ‰èƒ½è§£é–å„€è¡¨æ¿åŠŸèƒ½ã€‚")
    st.stop()

st.title(f"ğŸš€ {stock_id} å°è‚¡å…¨æ–¹ä½å„€è¡¨æ¿")

# ç²å–è³‡æ–™
with st.spinner(f"æ­£åœ¨èˆ‡è­‰äº¤æ‰€è¡›æ˜Ÿé€£ç·š... æ­£åœ¨æŠ“å– {stock_id} çš„è³‡æ–™..."):
    debug_log = []
    # é è¨­æŠ“ 2 å¹´
    hist_df, info, bs, is_, debug_log = get_market_package_data(stock_id, period_str="2y", custom_start_date=start_date)
    
    if hist_df.empty:
        st.error(f"âŒ æ‰¾ä¸åˆ° {stock_id} çš„è³‡æ–™ï¼è«‹ç¢ºå®šä»£è™Ÿæ²’æ‰“éŒ¯ï¼Œæˆ–æ˜¯è­‰äº¤æ‰€ä»Šå¤©æ”¾å‡å»äº†ã€‚")
        # é¡¯ç¤º Debug Log å¹«åŠ©é™¤éŒ¯
        with st.expander("ğŸ› ï¸ Debug Log (å¤±æ•—åŸå› )"):
            for log in debug_log: st.text(log)
        st.stop()
    
    stock_name = info.get('longName', stock_id)
    st.subheader(f"ç›®å‰æ¨™çš„: {stock_id} {stock_name}")

# Tabs
tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["ğŸ“Š è¡Œæƒ…èˆ‡åŸºæœ¬é¢", "ğŸ“ˆ æŠ€è¡“åˆ†æ", "ğŸ”® è‚¡åƒ¹é æ¸¬", "ğŸ’° ç±Œç¢¼èˆ‡é›†ä¿", "ğŸ“° æ–°è", "ğŸ” é¸è‚¡ç¯©é¸å™¨"])

# Tab 1: è¡Œæƒ…èˆ‡åŸºæœ¬é¢
with tab1:
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("### ğŸ“… æ—¥ K è¡Œæƒ…")
        st.dataframe(hist_df.sort_values('Date', ascending=False).head(50), height=300)
    
    with col2:
        st.markdown("### ğŸ¦ æœˆä¼°å€¼ (PE/PB/Yield)")
        monthly_df = get_official_monthly_valuation(stock_id)
        st.dataframe(monthly_df, height=300)
    
    st.markdown("---")
    c1, c2, c3 = st.columns(3)
    with c1: 
        st.markdown("### â„¹ï¸ å…¬å¸è³‡è¨Š")
        # ä¿®æ­£ PyArrow Error
        info_df = pd.DataFrame(list(info.items()), columns=["Key", "Value"])
        info_df["Value"] = info_df["Value"].astype(str)
        st.dataframe(info_df, height=300)

    with c2:
        st.markdown("### ğŸ“‰ è³‡ç”¢è² å‚µè¡¨")
        st.dataframe(bs, height=300)
    with c3:
        st.markdown("### ğŸ’¸ æç›Šè¡¨")
        st.dataframe(is_, height=300)
        
    st.markdown("### ğŸ§® è²¡å‹™æ¯”ç‡")
    ratios = calculate_fundamental_ratios(info, bs, is_)
    st.dataframe(ratios)

# Tab 2: æŠ€è¡“åˆ†æ
with tab2:
    col_ta1, col_ta2 = st.columns([1, 4])
    with col_ta1:
        ma_in = st.text_input("MA åƒæ•¸", value="5,10,20")
        heights_in = st.text_input("åœ–è¡¨é«˜åº¦æ¯”ä¾‹", value="0.7,0.3")
        inds_in = st.multiselect("æŠ€è¡“æŒ‡æ¨™", ["æˆäº¤é‡", "KD", "MACD", "RSI", "å¸ƒæ—é€šé“"], default=["æˆäº¤é‡"])
    with col_ta2:
        fig_ta = plot_technical_analysis(hist_df, inds_in, stock_id, ma_in, heights_in)
        if fig_ta:
            st.plotly_chart(fig_ta, use_container_width=True)

# Tab 3: è‚¡åƒ¹é æ¸¬
with tab3:
    st.markdown("### ğŸ”® AI é æ¸¬æ¨¡å‹ (Ridge Regression + Volatility Cloud)")
    summary, fig_pred = predict_stock_price(hist_df, predict_days=5, stock_id=stock_id, stock_name=stock_name)
    st.markdown(summary)
    if fig_pred:
        st.plotly_chart(fig_pred, use_container_width=True)

# Tab 4: ç±Œç¢¼èˆ‡é›†ä¿
with tab4:
    st.markdown("### ğŸ† é›†ä¿æˆ¶è‚¡æ¬Šåˆ†æ•£è¡¨ (Open Data)")
    tdcc_df = get_tdcc_opendata(stock_id)
    if not tdcc_df.empty:
        st.dataframe(tdcc_df)
        
        st.markdown("#### ğŸ’° æŒæœ‰åƒ¹å€¼è¨ˆç®—")
        c_tdcc1, c_tdcc2 = st.columns(2)
        with c_tdcc1:
            selected_brackets = st.multiselect("é¸æ“‡è¦è¨ˆç®—åƒ¹å€¼çš„ç´šè·", TDCC_BRACKETS, default=["400-600å¼µ", "600-800å¼µ", "800-1000å¼µ", "1000å¼µä»¥ä¸Š"])
        with c_tdcc2:
            threshold = st.number_input("å¤§æˆ¶é–€æª» (è¬å…ƒ)", value=10000, step=1000)
        
        if st.button("è¨ˆç®—å¤§æˆ¶åƒ¹å€¼"):
            val_df = calculate_tdcc_holding_value(tdcc_df, hist_df, selected_brackets, threshold)
            st.dataframe(val_df)
    else:
        st.info("æš«ç„¡é›†ä¿è³‡æ–™ (Open Data å¯èƒ½åªæä¾›æœ€æ–°ä¸€é€±)")

# Tab 5: æ–°è
with tab5:
    st.markdown("### ğŸ“° æœ€æ–°æ–°è")
    news_df = get_stock_news(stock_id, stock_name)
    if not news_df.empty:
        st.data_editor(
            news_df,
            column_config={"é€£çµ": st.column_config.LinkColumn("æ–°èé€£çµ")},
            disabled=True
        )
    else:
        st.write("æ²’æœ‰æ–°èï¼Œé€™å®¶å…¬å¸æœ€è¿‘å¯èƒ½å¾ˆä½èª¿ã€‚")

# Tab 6: é¸è‚¡ç¯©é¸å™¨
with tab6:
    st.markdown("### ğŸ” å¸‚å ´ç¯©é¸å™¨")
    with st.form("screener_form"):
        c_s1, c_s2, c_s3 = st.columns(3)
        with c_s1:
            mkt_type = st.radio("å¸‚å ´", ["ä¸Šå¸‚", "ä¸Šæ«ƒ"])
            pe_min = st.number_input("PE Min", value=0)
            pe_max = st.number_input("PE Max", value=20)
        with c_s2:
            yield_min = st.number_input("Yield Min (%)", value=3.0)
            f_trend = st.selectbox("å¤–è³‡å‹•å‘", ["ä¸é™", "å¢åŠ ", "æ¸›å°‘"])
            t_trend = st.selectbox("æŠ•ä¿¡å‹•å‘", ["ä¸é™", "å¢åŠ ", "æ¸›å°‘"])
        with c_s3:
            ta_ma_cond = st.selectbox("MA æ¢ä»¶", ["ä¸é™", "Price > MA5", "Price > MA20", "Price > MA60"])
            ta_rsi_cond = st.selectbox("RSI æ¢ä»¶", ["ä¸é™", "RSI > 50", "RSI < 50", "RSI < 30 (è¶…è³£)"])
        
        submit_screener = st.form_submit_button("ğŸš€ é–‹å§‹ç¯©é¸")
    
    if submit_screener:
        with st.spinner("æ­£åœ¨æƒæå…¨å°è‚¡å¸‚å ´... è«‹ç¨å€™ (é€™å¯èƒ½éœ€è¦ä¸€é»æ™‚é–“)..."):
            screen_result = get_market_screener_data(
                mkt_type, pe_min, pe_max, yield_min, 
                f_trend, t_trend, "ä¸é™", "ä¸é™", 
                ta_ma_cond, ta_rsi_cond
            )
            st.dataframe(screen_result)

# Debug Log (å¯é¸)
with st.expander("ğŸ› ï¸ Debug Log"):
    for log in debug_log:
        st.text(log)
